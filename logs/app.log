2025-07-25 11:08:32.195 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-25 11:11:21.769 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-25 11:11:33.225 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:11:34.345 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-25 11:11:45.519 | INFO     | src.models.openai_model:call_gpt4_with_full_text:23 - GPT-4 response time: 11.17 seconds
2025-07-25 11:13:09.281 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-25 11:13:26.079 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan mengenai midas model
2025-07-25 11:13:26.082 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:13:27.464 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-25 11:13:43.424 | INFO     | src.models.openai_model:call_gpt4_with_full_text:23 - GPT-4 response time: 15.96 seconds
2025-07-25 11:14:40.670 | INFO     | src.pipeline.rag_pipeline:__init__:14 - Creating new vector store at dataset/vector_store
2025-07-25 11:14:43.720 | INFO     | src.pipeline.rag_pipeline:__init__:21 - Adding chunks to vector store from docs/output.txt
2025-07-25 11:14:43.737 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:15:01.664 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa metodologi yang digunakan dalam skripsi ini?
2025-07-25 11:15:01.671 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:15:02.479 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-25 11:15:08.848 | INFO     | src.models.openai_model:call_gpt4_with_full_text:23 - GPT-4 response time: 6.37 seconds
2025-07-25 11:17:57.327 | INFO     | src.pipeline.rag_pipeline:__init__:14 - Creating new vector store at dataset/vector_store
2025-07-25 11:17:57.530 | INFO     | src.data.data_prep:extract_text_from_pdf:31 - Text extracted from 12 to 79 pages and saved to docs/output.txt
2025-07-25 11:18:00.332 | INFO     | src.pipeline.rag_pipeline:__init__:21 - Adding chunks to vector store from docs/output.txt
2025-07-25 11:18:00.345 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:18:17.089 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan hasil dari penelitian dalam skripsi ini
2025-07-25 11:18:17.097 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-25 11:18:17.884 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-25 11:18:29.143 | INFO     | src.models.openai_model:call_gpt4_with_full_text:23 - GPT-4 response time: 11.26 seconds
2025-07-26 21:48:48.797 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 21:49:05.121 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 21:49:08.920 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 21:49:25.092 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai skripsi saya
2025-07-26 21:49:25.094 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan mengenai skripsi saya
2025-07-26 21:49:25.097 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 21:49:28.383 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 21:49:38.433 | INFO     | src.models.openai_model:call_gpt4_with_full_text:23 - GPT-4 response time: 10.05 seconds
2025-07-26 21:49:38.433 | INFO     | __main__:rag_chat_interface:15 - Response from RAG pipeline: ```markdown
## Ringkasan

- Skripsi ini membahas proses deteksi objek menggunakan GPU, kemudian memanfaatkan NumPy untuk memindahkan hasil komputasi ke CPU agar dapat ditampilkan dengan mudah.
- Alur penelitian digambarkan pada Diagram Alir (Gambar 4.2).
- Terdapat dua tahapan utama:
  1. **Studi Literatur**  
     â€“ Mencari dan mempelajari jurnal relevan melalui ScienceDirect dan IEEE Xplore (langganan UGM).  
     â€“ Tujuan: membandingkan dengan penelitian terdahulu dan memperkuat hipotesis.  
  2. **Pengumpulan Data & Pemrosesan Awal**  
     â€“ Pembuatan dataset berupa foto instrumen bedah (bengkok, gunting anatomi, pinset).  
     â€“ Menggunakan kamera Samsung A15 5G, pengambilan gambar vertikal pada jarak 50 cm di atas meja.

---

## Teks yang Diperbaiki

### Proses Deteksi Objek  
Skripsi ini menjelaskan proses deteksi objek. Hasil inferensi pada GPU dipindahkan ke CPU menggunakan NumPy, sehingga memudahkan visualisasi deteksi objek dan estimasi urutan tampilan.

### Tata Laksana Penelitian  
Alur kerja penelitian ini ditunjukkan pada Gambar 4.2.

**Gambar 4.2.** Diagram Alir Tata Laksana Penelitian

#### IV.2.1. Studi Literatur  
Peneliti menelusuri literatur yang relevan dan mendukung dasar teori penelitian melalui jurnal elektronik, antara lain ScienceDirect dan IEEE Xplore (langganan Universitas Gadjah Mada). Tujuan pencarian literatur adalah untuk:  
- Membandingkan metode dan hasil dengan penelitian sebelumnya.  
- Memperkuat hipotesis yang diajukan.

#### IV.2.2. Pengumpulan Data dan Pemrosesan Awal Data  
Pembuatan dataset dilakukan dengan mengambil gambar instrumen bedah (bengkok, gunting anatomi, pinset) menggunakan kamera Samsung A15 5G. Pengambilan gambar dilakukan secara vertikal dengan jarak kamera 50 cm di atas meja, dan objek diletakkan di permukaan meja untuk memudahkan proses anotasi dan pemrosesan selanjutnya.
```
2025-07-26 22:04:08.558 | INFO     | __main__:<module>:39 - Gradio interface launched successfully
2025-07-26 22:04:15.540 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:04:19.774 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:04:46.281 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:04:49.978 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:05:05.248 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:05:07.997 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:05:24.088 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:05:25.615 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:05:45.082 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai latar belakang dari skripsi ini
2025-07-26 22:06:34.926 | INFO     | __main__:<module>:39 - Gradio interface launched successfully
2025-07-26 22:07:02.835 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:07:03.873 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:07:17.304 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai latar belakang skripsi saya
2025-07-26 22:07:52.319 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:07:53.791 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:08:12.184 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai latar belakang skripsi ini
2025-07-26 22:08:12.184 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan mengenai latar belakang skripsi ini
2025-07-26 22:08:12.185 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:08:14.820 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:08:19.492 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 4.67 seconds
2025-07-26 22:10:38.712 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:10:40.292 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:10:58.721 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai latar belakang yang mendasari penelitian ini
2025-07-26 22:10:58.721 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan mengenai latar belakang yang mendasari penelitian ini
2025-07-26 22:10:58.722 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:11:02.222 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:11:09.685 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 7.46 seconds
2025-07-26 22:11:39.390 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:11:40.338 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:11:54.257 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan mengenai latar belakang yang mendasari penelitian ini
2025-07-26 22:11:54.258 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan mengenai latar belakang yang mendasari penelitian ini
2025-07-26 22:11:54.258 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:11:56.194 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:12:06.721 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 10.53 seconds
2025-07-26 22:14:38.556 | INFO     | __main__:rag_chat_interface:13 - User input: Jelaskan kontribusi atau kebaruan yang dituliskan pada penelitian tersebut?
2025-07-26 22:14:38.557 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: Jelaskan kontribusi atau kebaruan yang dituliskan pada penelitian tersebut?
2025-07-26 22:14:38.562 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:14:41.187 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:14:45.859 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 4.67 seconds
2025-07-26 22:15:21.734 | INFO     | __main__:rag_chat_interface:13 - User input: Sebutkan dan jelaskan dataset yang digunakan pada penelitian ini
2025-07-26 22:15:21.735 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: Sebutkan dan jelaskan dataset yang digunakan pada penelitian ini
2025-07-26 22:15:21.742 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:15:23.349 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:15:29.030 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 5.68 seconds
2025-07-26 22:21:44.507 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan ke saya mengenai model yolov8
2025-07-26 22:21:44.508 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan ke saya mengenai model yolov8
2025-07-26 22:21:44.511 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:21:46.740 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:21:55.238 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 8.50 seconds
2025-07-26 22:23:36.352 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:23:40.601 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:24:09.619 | INFO     | __main__:rag_chat_interface:13 - User input: apa hasil akhir dari penelitian ini dan apa yang bisa dikembangkan dari skripsi ini
2025-07-26 22:24:09.619 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa hasil akhir dari penelitian ini dan apa yang bisa dikembangkan dari skripsi ini
2025-07-26 22:24:09.620 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:24:11.873 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.44052305817604065, 0.42692482471466064, 0.42581045627593994, 0.39956289529800415], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab54ce-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab6202-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', ' dalam penelitian ini:\n1. Batasan mencakup pengembangan sistem deteksi dan estimasi urutan\ntampilan menggunakan model YOLOv8 dan model MiDaS.\n2. Evaluasi sistem akan berfokus pada tingkat presisi dan waktu inferensi\ndalam mendeteksi alat operasi dan mengestimasi urutan tampilan antar\nelemen tersebut.\n3. Penelitian ini tidak akan membandingkan sistem yang diusulkan dengan\nteknik lain yang mungkin digunakan dalam konteks yang sama. Fokusnya\nadalah pada pengembangan sistem itu sendiri.\n4. Penelitian ini akan mempertimbangkan keterbatasan teknologi kamera\ndigital, model YOLOv8, dan model MiDaS yang mungkin mempengaruhi\nkinerja sistem.\nBatasan-batasan ini akan membantu mengarahkan penelitian untuk\nmencapai tujuan penyelesaian masalah yang lebih tepat dan fokus dalam\nkonteks pengembangan robot pembantu dokter untuk operasi bedah ringan.\nTujuan Penelitian\nTujuan dari penelitian ini adalah mengembangkan sistem deteksi dan\nestimasi urutan tampilan dalam konteks penggunaan untuk membantu dokter\n', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', 'ntara precision\ndan recall. Metriks terakhir adalah mAP yang memberikan ukuran keseluruhan\nkinerja model dengan memperhitungkan baik presisi maupun recall pada berbagai\nthreshold atau kelas.\nIV.2.6. Analisis dan Pembahasan\nAnalisis dan pembahasan penelitian ini mencakup analisis performa melalui\nmetrik akurasi yang telah dibahas pada tahap sebelumnya. Selain itu, tahap ini juga\nmembandingkan kombinasi varian YOLOv8 dan MiDaS yang paling efisien dan\noptimal untuk digunakan dalam sistem prediksi dan estimasi urutan tampilan.\nSelain itu, dalam analisis ini juga diidentifikasi pengaruh proses pra-pemrosesan\ngambar dan augmentasi terhadap metrik akurasi model. Struktur model YOLOv8\njuga dibedah untuk melihat bagaimana pengaruh infrastruktur model terhadap hasil\nprediksi. Hasil penelitian akan diolah melalui proses perangkuman hasil prediksi\nmenggunakan metrik akurasi yang disajikan dalam bentuk tabel dan statistik, serta\npengujian keterkaitan antara setiap variabel yang diajukan dalam penel']}
2025-07-26 22:24:11.873 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:24:24.432 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 12.56 seconds
2025-07-26 22:50:37.526 | INFO     | __main__:rag_chat_interface:13 - User input: apa latar belakang dari skripsi ini?
2025-07-26 22:50:37.528 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa latar belakang dari skripsi ini?
2025-07-26 22:50:37.555 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:50:40.304 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.39779773354530334, 0.37038710713386536, 0.36731472611427307, 0.36354145407676697], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab5f5a-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab5460-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', 'sebesar\n8 GB. Laptop ini berfungsi untuk menulis laporan penelitian, mencari literatur,\nmengolah data, menulis kode, melabeli gambar untuk pelatihan, serta menjalankan\nperangkat lunak yang digunakan dalam penelitian. Kamera digunakan untuk\nmengambil gambar yang akan digunakan sebagai data pelatihan dan data pengujian.\nPenelitian ini menggunakan kamera Samsung Galaxy A15 dengan resolusi gambar\nmaksimal yang bisa diambil adalah 50 megapiksel (MP) dengan kamera utama\nbelakang. Selain itu, kamera belakang juga memiliki dua lensa tambahan dengan\nresolusi 5 MP dan 2 MP yang digunakan untuk efek tambahan seperti depth sensing\natau makro.\nPenelitian ini menggunakan beberapa perangkat lunak dan pustaka\n(library), termasuk Google Chrome, Python, Google Colab, serta berbagai pustaka\nyang mendukung proses prapemrosesan gambar. Perangkat lunak yang paling\nsering digunakan adalah Google Chrome, peramban internet yang dikembangkan\noleh Google sejak tahun 2008. Chrome tersedia untuk berbagai sistem op', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', 'ot cross-dataset transfer yang mana\nmelatih sebuah model pada dataset tertentu dan melakukan testing pada dataset\nyang lain yang belum pernah dilihat selama proses belajar. Setelah evaluasi\nterhadap enam dataset berbeda, MiDaS mampu melebihi model yang sudah ada\nsecara kualitas dan kuantitas serta menjadi state of the art baru untuk estimasi\nurutan tampilan monocular.\nPerumusan Masalah\nPermasalahan yang diidentifikasi dalam tugas akhir ini berkaitan dengan\nkebutuhan untuk meningkatkan persepsi visual dokter selama melakukan prosedur\ntelesurgery. Akar permasalahannya adalah bahwa saat ini, dokter yang melakukan\ntelesurgery mengalami kesulitan visual bila hanya mengandalkan gambar 2D,\nsehingga bisa mengakibatkan miskoordinasi dalam menggerakkan lengan robot\npembantu. Oleh karena itu, permasalahan utama yang dibahas adalah bagaimana\nmembantu dokter memperoleh pemahaman yang lebih baik terhadap kondisi dan\nlingkungan selama telesurgery berlangsung.\nPendekatan penyelesaian yang diusulkan un']}
2025-07-26 22:50:40.304 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:50:48.763 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 8.46 seconds
2025-07-26 22:51:17.312 | INFO     | __main__:rag_chat_interface:13 - User input: apa permasalahan yang coba diselesaikan oleh skripsi ini?
2025-07-26 22:51:17.312 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa permasalahan yang coba diselesaikan oleh skripsi ini?
2025-07-26 22:51:17.314 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:51:19.285 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.4175432026386261, 0.41456836462020874, 0.39775949716567993, 0.38557055592536926], 'id': ['59ab5460-690e-11f0-951b-6e81b85efd07', '59ab549c-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab54ce-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': ['ot cross-dataset transfer yang mana\nmelatih sebuah model pada dataset tertentu dan melakukan testing pada dataset\nyang lain yang belum pernah dilihat selama proses belajar. Setelah evaluasi\nterhadap enam dataset berbeda, MiDaS mampu melebihi model yang sudah ada\nsecara kualitas dan kuantitas serta menjadi state of the art baru untuk estimasi\nurutan tampilan monocular.\nPerumusan Masalah\nPermasalahan yang diidentifikasi dalam tugas akhir ini berkaitan dengan\nkebutuhan untuk meningkatkan persepsi visual dokter selama melakukan prosedur\ntelesurgery. Akar permasalahannya adalah bahwa saat ini, dokter yang melakukan\ntelesurgery mengalami kesulitan visual bila hanya mengandalkan gambar 2D,\nsehingga bisa mengakibatkan miskoordinasi dalam menggerakkan lengan robot\npembantu. Oleh karena itu, permasalahan utama yang dibahas adalah bagaimana\nmembantu dokter memperoleh pemahaman yang lebih baik terhadap kondisi dan\nlingkungan selama telesurgery berlangsung.\nPendekatan penyelesaian yang diusulkan un', 'tuk mengatasi permasalahan ini\nadalah dengan mengimplementasikan sistem deteksi dan estimasi urutan tampilan\nmenggunakan model YOLOv8 dan MiDaS. Melalui pendekatan ini, sistem mampu\nmendeteksi objek dan memprediksi urutan tampilan antar objek. Informasi visual\ntersebut diharapkan dapat membantu dokter dalam memahami kondisi visual secara\nlebih menyeluruh, sehingga meningkatkan koordinasi gerakan saat mengendalikan\nlengan robot dari jarak jauh. Dengan demikian, pendekatan ini bertujuan untuk\nmeningkatkan akurasi dan keselamatan selama telesurgery.\nPendekatan penyelesaian ini akan membatasi cakupan permasalahan pada\npengembangan sistem untuk mendeteksi objek dan estimasi urutan tampilan, yang\n5\ndianggap langkah kritis dalam membantu dokter operator telesurgery menangkap\nlebih banyak informasi visual selama proses telesurgery. Dengan mengatasi akar\npermasalahan ini, diharapkan dapat memberikan kontribusi dalam bidang\nkesehatan.\nI.1.1. Batasan Masalah\nBerikut batasan masalah yang digunakan', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', ' dalam penelitian ini:\n1. Batasan mencakup pengembangan sistem deteksi dan estimasi urutan\ntampilan menggunakan model YOLOv8 dan model MiDaS.\n2. Evaluasi sistem akan berfokus pada tingkat presisi dan waktu inferensi\ndalam mendeteksi alat operasi dan mengestimasi urutan tampilan antar\nelemen tersebut.\n3. Penelitian ini tidak akan membandingkan sistem yang diusulkan dengan\nteknik lain yang mungkin digunakan dalam konteks yang sama. Fokusnya\nadalah pada pengembangan sistem itu sendiri.\n4. Penelitian ini akan mempertimbangkan keterbatasan teknologi kamera\ndigital, model YOLOv8, dan model MiDaS yang mungkin mempengaruhi\nkinerja sistem.\nBatasan-batasan ini akan membantu mengarahkan penelitian untuk\nmencapai tujuan penyelesaian masalah yang lebih tepat dan fokus dalam\nkonteks pengembangan robot pembantu dokter untuk operasi bedah ringan.\nTujuan Penelitian\nTujuan dari penelitian ini adalah mengembangkan sistem deteksi dan\nestimasi urutan tampilan dalam konteks penggunaan untuk membantu dokter\n']}
2025-07-26 22:51:19.285 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:51:28.290 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 9.00 seconds
2025-07-26 22:52:55.388 | INFO     | src.pipeline.rag_pipeline:__init__:11 - Using existing vector store at dataset/vector_store
2025-07-26 22:52:58.094 | INFO     | __main__:<module>:37 - Starting Gradio interface for RAG-based Document Assistant
2025-07-26 22:53:11.314 | INFO     | __main__:rag_chat_interface:13 - User input: apa yang menjadi tujuan dalam penelitian ini
2025-07-26 22:53:11.314 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa yang menjadi tujuan dalam penelitian ini
2025-07-26 22:53:11.315 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:53:13.889 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.4901667833328247, 0.4654058516025543, 0.45306557416915894, 0.4324999749660492], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab54ce-690e-11f0-951b-6e81b85efd07', '59ab5500-690e-11f0-951b-6e81b85efd07', '59ab5802-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', ' dalam penelitian ini:\n1. Batasan mencakup pengembangan sistem deteksi dan estimasi urutan\ntampilan menggunakan model YOLOv8 dan model MiDaS.\n2. Evaluasi sistem akan berfokus pada tingkat presisi dan waktu inferensi\ndalam mendeteksi alat operasi dan mengestimasi urutan tampilan antar\nelemen tersebut.\n3. Penelitian ini tidak akan membandingkan sistem yang diusulkan dengan\nteknik lain yang mungkin digunakan dalam konteks yang sama. Fokusnya\nadalah pada pengembangan sistem itu sendiri.\n4. Penelitian ini akan mempertimbangkan keterbatasan teknologi kamera\ndigital, model YOLOv8, dan model MiDaS yang mungkin mempengaruhi\nkinerja sistem.\nBatasan-batasan ini akan membantu mengarahkan penelitian untuk\nmencapai tujuan penyelesaian masalah yang lebih tepat dan fokus dalam\nkonteks pengembangan robot pembantu dokter untuk operasi bedah ringan.\nTujuan Penelitian\nTujuan dari penelitian ini adalah mengembangkan sistem deteksi dan\nestimasi urutan tampilan dalam konteks penggunaan untuk membantu dokter\n', 'operator telesurgery memperoleh lebih banyak informasi visual. Lebih khusus,\ntujuan utama adalah:\n1. Membangun sistem yang mampu mengidentifikasi dan melacak lokasi alat\nbedah.\n2. Membangun sistem yang mampu mengestimasi urutan tampilan antara alat\nbedah yang satu dengan yang lainnya.\n6\n3. Mendapatkan evaluasi hasil kerja kombinasi model deteksi objek dan\nestimasi urutan tampilan terbaik menggunakan YOLOv8 dan MiDaS.\nManfaat Penelitian\nManfaat dari penelitian ini adalah:\n1. Peningkatan akurasi dan presisi dalam prosedur telesurgery dengan\nmemungkinkan dokter untuk memiliki pemahaman yang lebih baik tentang\nlokasi alat bedah.\n2. Dokter operator telesurgery dapat bekerja dengan presisi tinggi, menghemat\nwaktu operasi, dan meminimalkan kerugian potensial yang dapat terjadi\nselama prosedur.\n3. Penelitian ini akan memberikan kontribusi dalam pengembangan teknologi\nmedis. Solusi yang diperkenalkan dalam penelitian ini dapat diterapkan\ndalam berbagai konteks telesurgery, yang pada gilirannya ', 'protokol: protokol keseluruhan yang menilai akurasi pada keseluruhan\npeta kedalaman, dan protokol berorientasi manusia yang mengukur akurasi hanya\npada wilayah yang mengandung manusia. Algoritma yang diusulkan memberikan\nhasil estimasi yang sebanding atau bahkan lebih baik dibandingkan dengan jaringan\ndasar pada dataset DIH dan HD P. Penulis menyoroti bahwa penelitian sebelumnya\nterutama berfokus pada perancangan arsitektur jaringan yang lebih baik dan\nmerumuskan fungsi loss yang efektif untuk estimasi urutan tampilan. Mereka juga\nmendiskusikan upaya untuk meningkatkan kinerja melalui penerapan regresi\nordinal, penerapan pembatasan geometris, pemisahan fitur tingkat rendah dan\ntinggi, serta penyeimbangan bobot untuk beberapa fungsi loss. Dalam makalah ini,\nditekankan kegunaan informasi mendalam dalam memberikan petunjuk tentang\ninformasi terkait sebuah adegan, seperti permukaan normal dan label segmentasi\nsemantik. Disebutkan bahwa penelitian ini mengembangkan algoritma yang\nmenangani ']}
2025-07-26 22:53:13.889 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:53:19.700 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 5.81 seconds
2025-07-26 22:53:49.455 | INFO     | __main__:rag_chat_interface:13 - User input: bagaimana solusi yang coba diberikan dalam penelitian ini?
2025-07-26 22:53:49.457 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: bagaimana solusi yang coba diberikan dalam penelitian ini?
2025-07-26 22:53:49.461 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:53:51.489 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.46301838755607605, 0.4266604781150818, 0.42220398783683777, 0.417049765586853], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab54ce-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab5500-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', ' dalam penelitian ini:\n1. Batasan mencakup pengembangan sistem deteksi dan estimasi urutan\ntampilan menggunakan model YOLOv8 dan model MiDaS.\n2. Evaluasi sistem akan berfokus pada tingkat presisi dan waktu inferensi\ndalam mendeteksi alat operasi dan mengestimasi urutan tampilan antar\nelemen tersebut.\n3. Penelitian ini tidak akan membandingkan sistem yang diusulkan dengan\nteknik lain yang mungkin digunakan dalam konteks yang sama. Fokusnya\nadalah pada pengembangan sistem itu sendiri.\n4. Penelitian ini akan mempertimbangkan keterbatasan teknologi kamera\ndigital, model YOLOv8, dan model MiDaS yang mungkin mempengaruhi\nkinerja sistem.\nBatasan-batasan ini akan membantu mengarahkan penelitian untuk\nmencapai tujuan penyelesaian masalah yang lebih tepat dan fokus dalam\nkonteks pengembangan robot pembantu dokter untuk operasi bedah ringan.\nTujuan Penelitian\nTujuan dari penelitian ini adalah mengembangkan sistem deteksi dan\nestimasi urutan tampilan dalam konteks penggunaan untuk membantu dokter\n', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', 'operator telesurgery memperoleh lebih banyak informasi visual. Lebih khusus,\ntujuan utama adalah:\n1. Membangun sistem yang mampu mengidentifikasi dan melacak lokasi alat\nbedah.\n2. Membangun sistem yang mampu mengestimasi urutan tampilan antara alat\nbedah yang satu dengan yang lainnya.\n6\n3. Mendapatkan evaluasi hasil kerja kombinasi model deteksi objek dan\nestimasi urutan tampilan terbaik menggunakan YOLOv8 dan MiDaS.\nManfaat Penelitian\nManfaat dari penelitian ini adalah:\n1. Peningkatan akurasi dan presisi dalam prosedur telesurgery dengan\nmemungkinkan dokter untuk memiliki pemahaman yang lebih baik tentang\nlokasi alat bedah.\n2. Dokter operator telesurgery dapat bekerja dengan presisi tinggi, menghemat\nwaktu operasi, dan meminimalkan kerugian potensial yang dapat terjadi\nselama prosedur.\n3. Penelitian ini akan memberikan kontribusi dalam pengembangan teknologi\nmedis. Solusi yang diperkenalkan dalam penelitian ini dapat diterapkan\ndalam berbagai konteks telesurgery, yang pada gilirannya ']}
2025-07-26 22:53:51.489 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:53:58.675 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 7.19 seconds
2025-07-26 22:54:39.294 | INFO     | __main__:rag_chat_interface:13 - User input: bagaimana hasil inferensi object menggunakan yolov8 dan midas? kombinasi mana yang paling baik?
2025-07-26 22:54:39.295 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: bagaimana hasil inferensi object menggunakan yolov8 dan midas? kombinasi mana yang paling baik?
2025-07-26 22:54:39.298 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:54:41.018 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.5674018263816833, 0.5536731481552124, 0.5498591065406799, 0.5412141680717468], 'id': ['59ab6450-690e-11f0-951b-6e81b85efd07', '59ab6482-690e-11f0-951b-6e81b85efd07', '59ab5212-690e-11f0-951b-6e81b85efd07', '59ab641e-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': ['dapat mempertahankan waktu pascaproses yang lebih\nrendah. Varian YOLOv8n dan YOLOv8s memiliki struktur yang lebih sederhana\ndan lebih sedikit parameter memungkinkan inferensi yang lebih cepat, cocok untuk\naplikasi yang memerlukan kccepatan deteksi dengan keterbatasan sumber daya\nkomputasi. YOLOv8m memiliki keseimbangan antara kompleksitas dan kecepatan,\nmenjadikannya pilihan yang baik untuk aplikasi yang memerlukan keseimbangan\nantara kecepatan dan akurasi. Varian YOLOv8l dan YOLOv8x memiliki struktur\nkompleks dengan lebih banyak lapisan dan parameter, cocok untuk aplikasi yang\nmemerlukan akurasi tinggi meskipun waktu inferensi yang lebih lama.\nPerbandingan Varian YOLOv8 dan MiDaS\nPenelitian ini menggunakan variasi YOLOv8 dan MiDaS untuk\nmendapatkan kombinasi yang paling baik diterapkan dalam sistem waktu nyata.\nLima varian dari YOLOv8 akan dikombinasikan dengan tiga varian MiDaS, yaitu\nSmall, Hybrid, dan Large. Jumlah total kombinasi YOLOv8 dan MiDaS adalah 15\nkombinasi yang ditunjukk', 'an pada Tabel 5.3. Kombinasi dua model antara YOLOv8\ndan MiDaS dilakukan secara asinkron. Untuk dapat menggunakan kedua model\nsecara asinkron, diperlukan pustaka â€˜nest_asyncioâ€™, yang berguna untuk\nmenjalankan proses inferensi kedua model\nsecara bersamaan.\nFungsi\nâ€˜detect_objectsâ€™ digunakan untuk mendeteksi objek dalam frame. Model deteksi\nobjek akan memberikan hasil berupa bounding box dan label kelas dari objek yang\nterdeteksi. Fungsi â€˜estimate_depthâ€™ digunakan untuk memperkirakan urutan\ntampilan dalam frame. Transformasi dilakukan pada frame, dan kemudian model\nMiDaS digunakan untuk menghasilkan peta kedalaman. Fungsi â€˜process_batchâ€™\nmenggabungkan deteksi objek dan estimasi urutan tampilan dalam satu batch. Hasil\ndeteksi objek dan peta kedalaman ditampilkan dalam frame. Fungsi â€˜mainâ€™ adalah\nfungsi utama yang menjalankan seluruh proses. Video dibuka, frame dibaca, dan\ndiproses dalam batch. Hasilnya kemudian disimpan ke video output.\n52\nTabel 5.3. Daftar kombinasi YOLOv8 dan MiDaS\nMiDaS', 'imation untuk estimasi urutan tampilan. Fokus utama penelitian adalah mencari\nkombinasi terbaik dari varian YOLOv8 dan MiDaS (tota 15 kombinasi)\nberdasarkan nilai Mean Average Precision (mAP) untuk deteksi objek dan waktu\npemrosesan keseluruhan sistem untuk inferensi.\nHasil penelitian menunjukkan bahwa sistem yang dirancang dengan\nYOLOv8 dan MiDaS berhasil mendeteksi objek, membedakan jenis objek\n(bengkok, gunting anatomis, pinset), dan memberikan estimasi urutan tampilan\nobjek dengan baik. Varian YOLOv8m mencapai performa deteksi terbaik dengan\nF1 score sebesar 0,97 pada confidence 0,635, serta mAP50 sebesar 0,995 untuk\nkelas bengkok. Secara keseluruhan, YOLOv8s dan YOLOv8m memberikan\nkombinasi presisi dan kinerja stabil dengan nilai mAP50 sebesar 0,988 dan 0,986\nmasing-masing. Waktu pemrosesan untuk YOLOv8m dengan MiDaS small adalah\n0,1149 detik dan untuk YOLOv8m dengan MiDaS small adalah 0,1486 detik.\nEvaluasi kualitas estimasi urutan tampilan MiDaS menggunakan Modulation\nTransfer F', 'cepat akan sangat diutamakan.\nBerdasarkan data, YOLOv8n dan YOLOv8m lebih disarankan karena memiliki\nwaktu inferensi yang paling singkat. Meskipun waktu pascaproses juga penting,\nvariabilitasnya tidak sebesar waktu inferensi. Namun, model dengan waktu\npascaproses yang lebih rendah seperti YOLOv8m juga dapat memberikan\nkeuntungan tambahan. Bila dikaitkan dengan struktur jaringan setiap varian,\nsemakin kompleks model (dengan lebih banyak lapisan dan parameter), semakin\nlama waktu yang diperlukan untuk melakukan inferensi. Oleh karena itu,\nYOLOv8x yang memiliki struktur paling kompleks membutuhkan waktu inferensi\nyang paling lama. Setelah inferensi, model harus memproses hasil deteksi,\nmenerapkan non-maximum supression (NMS), dan mengelompokkan bounding\nbox. Model dengan lebih banyak deteksi (karena akurasi yang lebih tinggi)\nmemerlukan waktu pascaproses yang lebih lama untuk menyaring dan\n51\nmengelompokkan hasil. Namun, dengan optimasi, model yang lebih besar seperti\nYOLOv8m dan YOLOv8l ']}
2025-07-26 22:54:41.018 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:54:49.935 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 8.92 seconds
2025-07-26 22:57:25.539 | INFO     | __main__:rag_chat_interface:13 - User input: jelaskan apa yang dimaksud dengan non maximum supression dalam penelitian ini?
2025-07-26 22:57:25.540 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: jelaskan apa yang dimaksud dengan non maximum supression dalam penelitian ini?
2025-07-26 22:57:25.542 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:57:27.845 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.4936054050922394, 0.41531622409820557, 0.4105125367641449, 0.3970535397529602], 'id': ['59ab5a8c-690e-11f0-951b-6e81b85efd07', '59ab5b18-690e-11f0-951b-6e81b85efd07', '59ab6202-690e-11f0-951b-6e81b85efd07', '59ab5a46-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': ['a bobot akan dijumlahkan menjadi satu.\nğ‘Šğ‘’ğ‘–ğ‘”â„ğ‘¡ğ‘’ğ‘‘ ğ‘šğ´ğ‘ƒ= âˆ‘\nğ‘Šğ‘–ğ´ğ‘ƒ(ğ‘–)\nğ¶\nğ‘–=1\nğ¶\n(3.6)\nKurva precision-recall membantu mengidentifikasi nilai ambang batas\nterbaik untuk deteksi dengan membuat plot precision dan recall untuk nilai ambang\nbatas yang berbeda. Area yang dicakup oleh kurva precision-recall adalah indikator\nkinerja lain. Semakin luas wilayah yang dicakup, semakin tinggi pula precision dan\nrecall-nya.\nIII.2.2. Non-Maximum Suppression\nNon-Maximum Suppression (NMS) merupakan algoritma pascaproses yang\numumnya digunakan dalam deteksi objek untuk menggabungkan sejumlah prediksi\n20\nobjek menjadi satu prediksi yang lebih akurat. NMS membantu mengatasi tumpeng\ntindih antara kotak pembatas, sehingga memilih kotak pembatas dengan nilai\nkepercayaan tertinggi untuk setiap objek dalam gambar. NMS memiliki peran\nkrusial dalam tiga tahap deteksi objek, yang melibatkan proposisi ruang pencarian\ndalam kotak pembatas, penyempurnaan kotak pembatas melalui proses\npengklasifikasian, dan penggabungan kotak', ' pembatas yang memprediksi objek\nyang sama. Proses kerja dari NMS dapat dijelaskan sebagai berikut:\n1. Input: Sekumpulan kotak pembatas yang telah dihasilkan oleh algoritma\ndeteksi objek, masing-masing dilengkapi dengan nilai kepercayaan\nprediksi.\n2. Pengurutan: Kotak pembatas diurutkan berdasarkan nilai kepercayaan\nprediksinya dari tertinggi ke terendah.\n3. Seleksi: Dimulai dari kotak pembatas dengan nilai kepercayaan tertinggi,\nNMS mempertahankan kotak tersebut dan menghapus kotak-kotak lain\nyang memiliki tumpang tindih signifikan dengan kotak yang telah dipilih.\n4. Iterasi: Langkah 3 diulang untuk kotak pembatas berikutnya dengan nilai\nkepercayaan yang lebih rendah, dan proses ini terus berlanjut hingga semua\nkotak pembatas dievaluasi.\n5. Output: Hasil akhir berupa subset kotak pembatas yang dipilih oleh NMS\nberdasarkan nilai kepercayaan tertinggi.\nDengan menerapkan NMS, diharapkan hasil deteksi objek lebih akurat dan\nbebas dari redundansi.\nIII.2.3. Arsitektur\nSeri algoritma YOLO me', 'ntara precision\ndan recall. Metriks terakhir adalah mAP yang memberikan ukuran keseluruhan\nkinerja model dengan memperhitungkan baik presisi maupun recall pada berbagai\nthreshold atau kelas.\nIV.2.6. Analisis dan Pembahasan\nAnalisis dan pembahasan penelitian ini mencakup analisis performa melalui\nmetrik akurasi yang telah dibahas pada tahap sebelumnya. Selain itu, tahap ini juga\nmembandingkan kombinasi varian YOLOv8 dan MiDaS yang paling efisien dan\noptimal untuk digunakan dalam sistem prediksi dan estimasi urutan tampilan.\nSelain itu, dalam analisis ini juga diidentifikasi pengaruh proses pra-pemrosesan\ngambar dan augmentasi terhadap metrik akurasi model. Struktur model YOLOv8\njuga dibedah untuk melihat bagaimana pengaruh infrastruktur model terhadap hasil\nprediksi. Hasil penelitian akan diolah melalui proses perangkuman hasil prediksi\nmenggunakan metrik akurasi yang disajikan dalam bentuk tabel dan statistik, serta\npengujian keterkaitan antara setiap variabel yang diajukan dalam penel', 'all = âˆ‘\nğŸ™\nğ‘›\nğ‘–=1 [ğ‘“(ğ‘¥ğ‘–) = 1 âˆ§ğ‘¦ğ‘–= 1]\nâˆ‘\nğŸ™\nğ‘›\nğ‘–=1 [ğ‘¦ğ‘–= 1]\n(3.2)\nAkurasi model diukur dengan menggunakan Average Precision (AP) yang\ndiestimasi melalui kurva precision-recall. Perbandingan antara berbagai model\ndeteksi objek dapat dilakukan menggunakan AP karena memberikan metrik\nnumerik, membuat perbandingan menjadi lebih mudah. AP dihitung secara efektif\ndengan menginterpolasi presisi melalui semua nilai recall r yang unik k, seperti\nyang diusulkan dalam Pascal VOC Challenge 2010.\nğ´ğ‘£ğ‘’ğ‘Ÿğ‘ğ‘”ğ‘’ ğ‘ƒğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘›= âˆ‘ğ‘ƒğ‘–(ğ‘Ÿğ‘˜)\nğ‘˜=0\n(3.3)\nğ‘ƒğ‘–(ğ‘Ÿğ‘˜) = max(ğ‘(ğ‘˜Ì‚))\n(3.4)\nDi mana ğ‘ƒğ‘–(ğ‘Ÿğ‘˜) merupakan interpolasi presisi yang menghasilkan presisi\nmaksimum pada semua recall yang lebih besar dari k. Adapun mean Average\nPrecision (mAP) adalah rata-rata AP dari seluruh representasi kelas dan dapat\ndihitung dengan formula berikut:\nğ‘šğ´ğ‘ƒ= âˆ‘\nğ´ğ‘ƒ(ğ‘–)\nğ¶\nğ‘–=1\nğ¶\n(3.5)\nRata-rata tertimbang dari AP digunakan apabila terdapat perbedaan jumlah\nsampel di setiap kelas. Untuk semua kelas C, bobot ğ‘¤ğ‘– diberikan berdasarkan\njumlah sampel kelas. Semu']}
2025-07-26 22:57:27.847 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:57:33.144 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 5.30 seconds
2025-07-26 22:58:06.300 | INFO     | __main__:rag_chat_interface:13 - User input: berapa jumlah data yang digunakan dalam proses training di dalam penelitian ini?
2025-07-26 22:58:06.301 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: berapa jumlah data yang digunakan dalam proses training di dalam penelitian ini?
2025-07-26 22:58:06.304 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:58:07.870 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.5095539689064026, 0.505364716053009, 0.4887855350971222, 0.4792378842830658], 'id': ['59ab65ea-690e-11f0-951b-6e81b85efd07', '59ab6126-690e-11f0-951b-6e81b85efd07', '59ab56cc-690e-11f0-951b-6e81b85efd07', '59ab60f4-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': ['lah anotasi dalam dataset.\nSebagian besar data memiliki satu anotasi (90 data), diikuti oleh data dengan 2\nanotasi (60 data), dan data dengan 3 anotasi yang paling sedikit (30 data). Dataset\nini memiliki lebih banyak data dengan anotasi tunggal dibandingkan data dengan\nanotasi ganda atau lebih. Hal ini menunjukkan bahwa sebagian besar objek dalam\ndataset relatif sederhana.\nSetelah prapemrosesan, gambar yang telah terlabeli kemudian dilakukan\naugmentasi. Jenis augmentasi yang diterapkan pada dataset penelitian ini yaitu: flip\n(pembalikan), rotasi, dan noise. Pemilihan ketiga augmentasi tersebut memastikan\nbahwa model dapat menangani berbagai varian dalam data, yang penting untuk\ngeneralisasi yang baik. Model tidak akan hanya bekerja dengan baik pada gambar\nyang mirip dengan data latih tetapi juga pada gambar yang mungkin memiliki\nsedikit perbedaan dalam orientasi, posisi, dan kualitas. Augmentasi data ini secara\nkeseluruhan meningkatkan akurasi dan robustness dari model deteksi dan esti', 'n perangkat lunak. Proses utama dari tahap ini adalah penulisan kode\nuntuk deteksi objek dan estimasi urutan tampilan menggunakan bahasa\npemrograman Python pada Google Colab Pro, dengan kerangka kerja utama yang\ndigunakan adalah YOLOv8 dan MiDaS.\nUntuk menggunakan dataset yang sudah dianotasi dalam proses pelatihan,\nRoboflow digunakan untuk memudahkan proses pembagian dataset menjadi data\nlatih, data validasi, dan data uji. Proses di Roboflow berjalan secara otomatis\n41\ndengan cara membuat proyek baru di ruang kerja Roboflow. Setelah berhasil\nmembuat proyek baru, gambar dan hasil anotasi label dari Makesense.ai diunggah\nke Roboflow. Pengaturan di Roboflow yang dapat disesuaikan meliputi\nperbandingan antara jumlah data latih, data validasi, dan data uji. Selain itu, dapat\njuga dipilih pre-processing dan augmentasi yang akan digunakan. Dalam penelitian\nini, augmentasi data yang digunakan adalah flip, rotasi, dan noise. Augmentasi\nflipping adalah teknik augmentasi di mana gambar dibalik s', '24 piksel (vertikal). Dataset kemudian diberi label secara\nmanual dengan menggunakan LabelImg, memastikan bahwa pusat dari setiap\nbunga berada di tengah kotak pembatas. Dari dataset yang disiapkan yang telah\ndisiapkan, sebanyak 81% (1.962 gambar) digunakan sebagai data pelatihan, 9%\n(219 gambar) untuk verifikasi, dan 10% (243 gambar) digunakan sebagai data\npengujian. Algoritma telah diujicoba pada berbagai dataset, termasuk dataset bunga\nstroberi, dataset Tomato, dataset deteksi Turbin Angin, dan dataset VOC2007. Hasil\neksperimen menunjukkan adanya peningkatan dalam mAP, recall, F1 score, dan\nwaktu inferensi.\nUrutan Tampilan Tiga Dimensi\nLee dan Park (2021) mengajukan metode untuk memperkirakan urutan\ntampilan video yang stabil secara temporer dari serangkaian gambar menggunakan\n11\njaringan yang dikenal sebagai STAD (Stable Video Depth Estimation). Metode\nyang\ndiusulkan\nmengintegrasikan\nmodul\ntemporal\nattention,\noperasi\npembengkokan geometris, dan scale-invariant loss guna meningkatkan', 'ja.\nMasing-masing objek diambil gambarnya sebanyak 60 kali, sehingga total gambar\nyang perlu dianotasi adalah 180 gambar. Proses pembangunan dataset dilakukan\ndengan anotasi gambar di laman Makesense.ai. Makesense.ai adalah laman daring\nuntuk melakukan anotasi gambar yang digunakan untuk tugas visi komputer seperti\n40\ndeteksi objek dan segmentasi gambar. Gambar 4.3 menunjukkan tampilan\nMakesense.ai.\nGambar 4.3. Halaman editor Makesense.ai\nFormat yang dapat diekspor dari hasil anotasi di Makesense.ai adalah format\nYOLO, VOC XML, dan CSV. Untuk penelitian ini, dipilih format YOLO agar\nsesuai dengan model yang akan digunakan, yaitu YOLOv8.\nIV.2.3. Pengembangan dan Konstruksi Sistem\nProses ini diawali dengan pengembangan yang mencakup penetapan\ntuntutan rancangan yang ingin dicapai oleh sistem dan penyusunan diagram alirnya.\nSetelah itu, proses dilanjutkan dengan melakukan konstruksi atau pembangunan\nsistem berdasarkan rancangan yang telah dibuat, yang mencakup sistem perangkat\nkeras maupu']}
2025-07-26 22:58:07.870 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:58:13.180 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 5.31 seconds
2025-07-26 22:58:27.265 | INFO     | __main__:rag_chat_interface:13 - User input: apa judul penelitian ini?
2025-07-26 22:58:27.266 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: apa judul penelitian ini?
2025-07-26 22:58:27.266 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:58:28.552 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.4703826606273651, 0.4481604993343353, 0.43914496898651123, 0.43764638900756836], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab54ce-690e-11f0-951b-6e81b85efd07', '59ab5802-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', ' dalam penelitian ini:\n1. Batasan mencakup pengembangan sistem deteksi dan estimasi urutan\ntampilan menggunakan model YOLOv8 dan model MiDaS.\n2. Evaluasi sistem akan berfokus pada tingkat presisi dan waktu inferensi\ndalam mendeteksi alat operasi dan mengestimasi urutan tampilan antar\nelemen tersebut.\n3. Penelitian ini tidak akan membandingkan sistem yang diusulkan dengan\nteknik lain yang mungkin digunakan dalam konteks yang sama. Fokusnya\nadalah pada pengembangan sistem itu sendiri.\n4. Penelitian ini akan mempertimbangkan keterbatasan teknologi kamera\ndigital, model YOLOv8, dan model MiDaS yang mungkin mempengaruhi\nkinerja sistem.\nBatasan-batasan ini akan membantu mengarahkan penelitian untuk\nmencapai tujuan penyelesaian masalah yang lebih tepat dan fokus dalam\nkonteks pengembangan robot pembantu dokter untuk operasi bedah ringan.\nTujuan Penelitian\nTujuan dari penelitian ini adalah mengembangkan sistem deteksi dan\nestimasi urutan tampilan dalam konteks penggunaan untuk membantu dokter\n', 'protokol: protokol keseluruhan yang menilai akurasi pada keseluruhan\npeta kedalaman, dan protokol berorientasi manusia yang mengukur akurasi hanya\npada wilayah yang mengandung manusia. Algoritma yang diusulkan memberikan\nhasil estimasi yang sebanding atau bahkan lebih baik dibandingkan dengan jaringan\ndasar pada dataset DIH dan HD P. Penulis menyoroti bahwa penelitian sebelumnya\nterutama berfokus pada perancangan arsitektur jaringan yang lebih baik dan\nmerumuskan fungsi loss yang efektif untuk estimasi urutan tampilan. Mereka juga\nmendiskusikan upaya untuk meningkatkan kinerja melalui penerapan regresi\nordinal, penerapan pembatasan geometris, pemisahan fitur tingkat rendah dan\ntinggi, serta penyeimbangan bobot untuk beberapa fungsi loss. Dalam makalah ini,\nditekankan kegunaan informasi mendalam dalam memberikan petunjuk tentang\ninformasi terkait sebuah adegan, seperti permukaan normal dan label segmentasi\nsemantik. Disebutkan bahwa penelitian ini mengembangkan algoritma yang\nmenangani ']}
2025-07-26 22:58:28.553 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:58:35.301 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 6.75 seconds
2025-07-26 22:59:05.302 | INFO     | __main__:rag_chat_interface:13 - User input: bagaimana solusi yang diberikan oleh penulis dalam penelitian ini?
2025-07-26 22:59:05.302 | INFO     | src.pipeline.rag_pipeline:query:26 - User query: bagaimana solusi yang diberikan oleh penulis dalam penelitian ini?
2025-07-26 22:59:05.309 | INFO     | src.models.embedding_model:embedding_function:6 - Generating embeddings for texts
2025-07-26 22:59:06.337 | INFO     | src.pipeline.rag_pipeline:query:28 - Search results: {'score': [0.4175955057144165, 0.41582491993904114, 0.4141067862510681, 0.38604736328125], 'id': ['59ab60c2-690e-11f0-951b-6e81b85efd07', '59ab6248-690e-11f0-951b-6e81b85efd07', '59ab5802-690e-11f0-951b-6e81b85efd07', '59ab5f5a-690e-11f0-951b-6e81b85efd07'], 'metadata': [{'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}, {'source': 'docs/output.txt'}], 'text': [' proses deteksi objek. NumPy membantu\nmengubah hasil dari GPU ke CPU untuk menampilkan hasil deteksi objek dan\nestimasi urutan tampilan.\nTata Laksana Penelitian\nTata laksana penelitian ini ditunjukkan pada Gambar 4.2.\n39\nGambar 4.2. Diagram alir tata laksana penelitian\nIV.2.1. Studi Literatur\nDalam penelitian ini, peneliti mencari literatur yang berkaitan dan\nmendukung dasar teori penelitian melalui laman jurnal seperti ScienceDirect dan\nIEEE Xplore yang dilanggan oleh Universitas Gadjah Mada. Proses pencarian\nliteratur ini bertujuan untuk membandingkan penelitian ini dengan penelitian\nterdahulu dan untuk memperkuat hipotesis yang diajukan oleh peneliti.\nIV.2.2. Pengumpulan Data dan Pemrosesan Awal Data\nPembangunan dataset dilakukan dengan proses pengambilan gambar\nbengkok, gunting anatomi, dan pinset menggunakan kamera Samsung A15 5G.\nPengambilan gambar dilakukan secara vertikal dengan kamera berada di atas meja\nsejauh 50 cm, dan objek yang akan diambil gambarnya diletakkan di atas me', 'itian ini.\nIV.2.7. Penulisan Laporan\nSesuai dengan format laporan dari Departemen Teknik Nuklir dan Teknik\nFisika (DTNTF) Universitas Gadjah Mada, penulisan laporan terdiri dari enam bab,\nyaitu:\n1. Pendahuluan\n2. Tinjauan Pustaka\n3. Dasar Teori\n4. Pelaksanaan Penelitian\n5. Hasil dan Pembahasan\n6. Kesimpulan dan Saran\n44\nHASIL DAN PEMBAHASAN\nDalam penelitian ini, metrik akurasi yang digunakan untuk mengevaluasi\nperforma sistem adalah metrik mean Average Precision (mAP). Metrik mAP dipilih\nkarena metrik ini menggabungkan antara presisi (seberapa banyak prediksi yang\nrelevan) dengan recall (seberapa banyak data relevan yang berhasil dideteksi).\nPenggabungan presisi dan recall menunjukkan keseimbangan performa sistem.\nMetrik mAP juga cocok untuk mengevaluasi sistem dalam penelitian ini karena\nmampu menangani sistem dengan banyak kelas. Kelas yang digunakan dalam\npenelitian ini terdiri dari 3 kelas yaitu kelas bengkok, kelas gunting anatomis, dan\nkelas pinset. Kemampuan mAP untuk mengevalua', 'protokol: protokol keseluruhan yang menilai akurasi pada keseluruhan\npeta kedalaman, dan protokol berorientasi manusia yang mengukur akurasi hanya\npada wilayah yang mengandung manusia. Algoritma yang diusulkan memberikan\nhasil estimasi yang sebanding atau bahkan lebih baik dibandingkan dengan jaringan\ndasar pada dataset DIH dan HD P. Penulis menyoroti bahwa penelitian sebelumnya\nterutama berfokus pada perancangan arsitektur jaringan yang lebih baik dan\nmerumuskan fungsi loss yang efektif untuk estimasi urutan tampilan. Mereka juga\nmendiskusikan upaya untuk meningkatkan kinerja melalui penerapan regresi\nordinal, penerapan pembatasan geometris, pemisahan fitur tingkat rendah dan\ntinggi, serta penyeimbangan bobot untuk beberapa fungsi loss. Dalam makalah ini,\nditekankan kegunaan informasi mendalam dalam memberikan petunjuk tentang\ninformasi terkait sebuah adegan, seperti permukaan normal dan label segmentasi\nsemantik. Disebutkan bahwa penelitian ini mengembangkan algoritma yang\nmenangani ', 'sebesar\n8 GB. Laptop ini berfungsi untuk menulis laporan penelitian, mencari literatur,\nmengolah data, menulis kode, melabeli gambar untuk pelatihan, serta menjalankan\nperangkat lunak yang digunakan dalam penelitian. Kamera digunakan untuk\nmengambil gambar yang akan digunakan sebagai data pelatihan dan data pengujian.\nPenelitian ini menggunakan kamera Samsung Galaxy A15 dengan resolusi gambar\nmaksimal yang bisa diambil adalah 50 megapiksel (MP) dengan kamera utama\nbelakang. Selain itu, kamera belakang juga memiliki dua lensa tambahan dengan\nresolusi 5 MP dan 2 MP yang digunakan untuk efek tambahan seperti depth sensing\natau makro.\nPenelitian ini menggunakan beberapa perangkat lunak dan pustaka\n(library), termasuk Google Chrome, Python, Google Colab, serta berbagai pustaka\nyang mendukung proses prapemrosesan gambar. Perangkat lunak yang paling\nsering digunakan adalah Google Chrome, peramban internet yang dikembangkan\noleh Google sejak tahun 2008. Chrome tersedia untuk berbagai sistem op']}
2025-07-26 22:59:06.338 | INFO     | src.models.openai_model:call_gpt4_with_full_text:11 - Calling GPT-4 with full text input
2025-07-26 22:59:11.071 | INFO     | src.models.openai_model:call_gpt4_with_full_text:24 - GPT-4 response time: 4.73 seconds
